import gradio as gr
from ultralytics import YOLO
import cv2
import numpy as np
from pydantic import ConfigDict

model_config = ConfigDict(arbitrary_types_allowed=True)

NAMES = ['NOTEBOOK', 'paper', 'pen', 'post-it', 'bottle', 'cup', 'laptop', 'mouse', 'keyboard']

organizer_classes = {"NOTEBOOK", "book"}
unnecessary_classes = {"paper", "post-it"}
device_sets = [{"laptop", "keyboard", "mouse"}]
messy_classes = {"paper", "post-it", "bottle"}

def compute_iou(box1, box2):
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    inter_area = max(0, x2 - x1) * max(0, y2 - y1)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = box1_area + box2_area - inter_area
    return inter_area / union_area if union_area > 0 else 0

def extract_visual_features(image_gray, image):
    h, w = image_gray.shape
    laplacian = cv2.Laplacian(image_gray, cv2.CV_64F)
    edge_strength = np.mean(np.abs(laplacian))

    hist = cv2.calcHist([image], [0, 1, 2], None, [8, 8, 8], [0, 256]*3)
    hist_norm = hist / hist.sum()
    color_entropy = -np.sum(hist_norm * np.log2(hist_norm + 1e-6))

    block_size = 50
    block_var = []
    for y in range(0, h, block_size):
        for x in range(0, w, block_size):
            block = image_gray[y:y+block_size, x:x+block_size]
            if block.size > 0:
                block_var.append(np.var(block))
    mean_variance = np.mean(block_var)

    edges = cv2.Canny(image_gray, 100, 200)
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    contour_count = len(contours)

    blurred = cv2.GaussianBlur(image_gray, (9, 9), 0)
    texture_diff = np.mean(np.abs(image_gray - blurred))

    return edge_strength, color_entropy, mean_variance, contour_count, texture_diff

def compute_combined_overlap_score(iou_scores, edge_map, detections, image_shape):
    n = len(detections)
    total_pairs = n * (n - 1) / 2 if n > 1 else 0
    if total_pairs == 0:
        return 0.0

    iou_overlap_count = np.sum(iou_scores >= 0.5) / 2
    iou_overlap_ratio = iou_overlap_count / total_pairs

    edge_overlap_score = 0
    for i in range(n):
        for j in range(i + 1, n):
            if iou_scores[i, j] >= 0.5:
                box1 = detections[i]['box']
                box2 = detections[j]['box']
                x1 = int(max(box1[0], box2[0]))
                y1 = int(max(box1[1], box2[1]))
                x2 = int(min(box1[2], box2[2]))
                y2 = int(min(box1[3], box2[3]))
                if x2 > x1 and y2 > y1:
                    overlap_region = edge_map[y1:y2, x1:x2]
                    edge_density = np.mean(overlap_region) / 255
                    edge_overlap_score += edge_density

    edge_overlap_score /= total_pairs
    combined_score = 0.7 * iou_overlap_ratio + 0.3 * edge_overlap_score
    return combined_score

def is_aligned(points, tolerance_deg=15):
    # points: [(x1, y1), (x2, y2), (x3, y3)]
    if len(points) < 3:
        return False
    a, b, c = np.array(points[0]), np.array(points[1]), np.array(points[2])
    v1 = b - a
    v2 = c - b
    if np.linalg.norm(v1) == 0 or np.linalg.norm(v2) == 0:
        return False
    cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
    angle = np.arccos(np.clip(cos_angle, -1, 1)) * 180 / np.pi
    return abs(angle) < tolerance_deg or abs(angle - 180) < tolerance_deg

def score_desk(detections, image_gray, image):
    score = 80
    feedback = []
    total_pairs = len(detections) * (len(detections) - 1) / 2 if len(detections) > 1 else 0

    # ì¤‘ë³µ ë¬¼ê±´ ì²´í¬
    item_counts = {}
    for d in detections:
        item_counts[d['name']] = item_counts.get(d['name'], 0) + 1
    
    duplicate_items = {item: count for item, count in item_counts.items() if count > 1}
    if duplicate_items:
        score -= 10
        feedback.append(f"ğŸ“Œ ê°™ì€ ë¬¼ê±´ì´ ì—¬ëŸ¬ ê°œ ìˆìŠµë‹ˆë‹¤: {', '.join([f'{item}({count}ê°œ)' for item, count in duplicate_items.items()])} (ê°ì  -10)")
        feedback.append("ğŸ’¡ ì •ë¦¬ íŒ: ê°™ì€ ì¢…ë¥˜ì˜ ë¬¼ê±´ì€ í•˜ë‚˜ë§Œ ì±…ìƒ ìœ„ì— ë‘ê³ , ë‚˜ë¨¸ì§€ëŠ” ì •ë¦¬í•˜ì„¸ìš”.")

    # ì§€ì €ë¶„í•œ ë¬¼ê±´ ì²´í¬
    messy_items = [d['name'] for d in detections if d['name'] in messy_classes]
    if messy_items:
        score -= 15
        feedback.append(f"ğŸ“Œ ì§€ì €ë¶„í•œ ë¬¼ê±´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(messy_items)} (ê°ì  -15)")
        feedback.append("ğŸ’¡ ì •ë¦¬ íŒ: ë¶ˆí•„ìš”í•œ ì¢…ì´, í¬ìŠ¤íŠ¸ì‡, ë¬¼ë³‘ì„ ì •ë¦¬í•˜ê±°ë‚˜ íŒŒì¼ì— ë³´ê´€í•˜ì„¸ìš”.")

    # ë¬¼ê±´ ê°„ ê²¹ì¹¨ ì²´í¬ (ìƒˆë¡œìš´ ë°©ì‹)
    n = len(detections)
    iou_matrix = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            if i != j:
                iou_matrix[i, j] = compute_iou(detections[i]['box'], detections[j]['box'])

    edge_map = cv2.Canny(image_gray, 100, 200)
    combined_overlap_score = compute_combined_overlap_score(iou_matrix, edge_map, detections, image_gray.shape)

    # ë³µí•© ê²¹ì¹¨ ì ìˆ˜ ë° ì‹œê°ì  ë³µì¡ë„ ë³´ì •
    if combined_overlap_score > 0.15:
        score -= 10
        feedback.append("ğŸ“Œ ë¬¼ê±´ ê°„ ê²¹ì¹¨ì´ ì‹¬í•˜ê³  ì–´ìˆ˜ì„ í•´ ë³´ì…ë‹ˆë‹¤. (ê°ì  -10)")
        feedback.append("ğŸ’¡ ì •ë¦¬ íŒ: ê²¹ì³ì§„ ë¬¼ê±´ë“¤ì„ ë¶„ë¦¬í•˜ê³  ì—¬ë°±ì„ í™•ë³´í•˜ì„¸ìš”.")
    else:
        edge_strength, color_entropy, mean_variance, contour_count, texture_diff = extract_visual_features(image_gray, image)
        if edge_strength > 10 or contour_count > 1000:
            score -= 7
            feedback.append("ğŸ“Œ ê²¹ì¹¨ì€ ì ì§€ë§Œ, ì±…ìƒì´ ì‹œê°ì ìœ¼ë¡œ ì–´ìˆ˜ì„ í•©ë‹ˆë‹¤. (ê°ì  -7)")
            feedback.append("ğŸ’¡ ì •ë¦¬ íŒ: ë¬¼ê±´ì„ ì •ë ¬í•˜ê±°ë‚˜, ë¶ˆí•„ìš”í•œ ë¬¼ê±´ì„ ì¹˜ì›Œë³´ì„¸ìš”.")

    # ì „ìê¸°ê¸° ì •ë ¬ ì²´í¬
    centers = {d['name']: ((d['box'][0]+d['box'][2])/2, (d['box'][1]+d['box'][3])/2) for d in detections}

    # í‚¤ë³´ë“œ-ë§ˆìš°ìŠ¤ ì •ë ¬ (ì•„ì£¼ ê´€ëŒ€í•˜ê²Œ, ë” ë„ë„í•˜ê²Œ)
    if 'keyboard' in centers and 'mouse' in centers:
        k, m = centers['keyboard'], centers['mouse']
        dx = abs(k[0] - m[0])
        dy = abs(k[1] - m[1])
        if dy < 120 or dx < 200:
            score += 5
            feedback.append('âœ… í‚¤ë³´ë“œì™€ ë§ˆìš°ìŠ¤ê°€ ì˜ ì •ë ¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ê°€ì  +5)')
        else:
            score -= 2  # ê°ì ë„ ì¤„ì—¬ì¤Œ
            feedback.append('ğŸ“Œ í‚¤ë³´ë“œì™€ ë§ˆìš°ìŠ¤ê°€ ë‹¤ì†Œ ë–¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. (ê°ì  -2)')
            feedback.append('ğŸ’¡ ì •ë¦¬ íŒ: í‚¤ë³´ë“œì™€ ë§ˆìš°ìŠ¤ë¥¼ ì¢€ ë” ê°€ê¹Œì´ ë†“ì•„ë³´ì„¸ìš”.')

    # ì£¼ìš” ì „ìê¸°ê¸° ì •ë ¬ ì²´í¬ (ê´€ëŒ€í•œ ë²„ì „)
    main_items = []
    for name in ['laptop', 'keyboard', 'mouse']:
        if name in centers:
            main_items.append((name, centers[name]))

    if len(main_items) == 3:
        points = [item[1] for item in main_items]

        def loose_alignment(p):
            x_coords, y_coords = zip(*p)
            x_range = max(x_coords) - min(x_coords)
            y_range = max(y_coords) - min(y_coords)
            # ê°€ë¡œ ë˜ëŠ” ì„¸ë¡œë¡œ ëŒ€ëµ ë¹„ìŠ·í•œ ì„ ìƒì— ìˆìœ¼ë©´ OK
            return x_range < 400 or y_range < 150

        if loose_alignment(points):
            score += 5
            feedback.append('âœ… ì „ìê¸°ê¸° ë°°ì¹˜ê°€ ëŒ€ì²´ë¡œ ì˜ ì •ë ¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. (ê°€ì  +5)')
        else:
            score -= 2
            feedback.append('ğŸ“Œ ì „ìê¸°ê¸° ìœ„ì¹˜ê°€ ì¡°ê¸ˆ ì–´ìˆ˜ì„ í•©ë‹ˆë‹¤. (ê°ì  -2)')
            feedback.append('ğŸ’¡ ì •ë¦¬ íŒ: í‚¤ë³´ë“œì™€ ë§ˆìš°ìŠ¤ë¥¼ ë…¸íŠ¸ë¶ê³¼ ë¹„ìŠ·í•œ ì„ ìƒì— ë°°ì¹˜í•´ë³´ì„¸ìš”.')

    # ë¬¼ê±´ ìˆ˜ ì²´í¬
    if len(detections) >= 8:
        score -= 5
        feedback.append("ğŸ“Œ íƒì§€ ê°ì²´ ìˆ˜ê°€ ë§ìŠµë‹ˆë‹¤. (ê°ì  -5)")
        feedback.append("ğŸ’¡ ì •ë¦¬ íŒ: í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë¬¼ê±´ë“¤ì€ ì±…ìƒì—ì„œ ì¹˜ìš°ê³ , í•„ìš”í•œ ê²ƒë§Œ ë‚¨ê²¨ë‘ì„¸ìš”.")

    # ì „ìê¸°ê¸° ì„¸íŠ¸ ì²´í¬
    detected_names = set([d['name'] for d in detections])
    for device in device_sets:
        if device.issubset(detected_names):
            score += 5
            feedback.append("âœ… ì „ìê¸°ê¸° ì„¸íŠ¸ ê°ì§€ë¨ (ê°€ì  +5)")
            break

    # ë¬¼ê±´ ê°„ ê²¹ì¹¨ ì²´í¬ (ì¤‘ê°„)
    iou_mid = sum(1 for i in range(len(detections)) for j in range(i+1, len(detections))
                  if compute_iou(detections[i]['box'], detections[j]['box']) >= 0.3)
    if total_pairs > 0 and (iou_mid / total_pairs) <= 0.1:
        score += 5
        feedback.append("âœ… íƒì§€ëœëœ ë¬¼ê±´ ê°„ ê²¹ì¹¨ì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. (ê°€ì  +5)")

    # ì‹œê°ì  ë³µì¡ë„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
    edge_score = np.clip((15 - edge_strength) / 15, 0, 1)
    entropy_score = np.clip((4.0 - color_entropy) / 4.0, 0, 1)
    variance_score = np.clip((3000 - mean_variance) / 3000, 0, 1)
    contour_score = np.clip((1600 - contour_count) / 1600, 0, 1)
    texture_score = np.clip((90 - texture_diff) / 90, 0, 1)

    vision_score = (
        0.2 * edge_score +
        0.2 * entropy_score +
        0.2 * variance_score +
        0.2 * contour_score +
        0.2 * texture_score
    )
    vision_score_scaled = (vision_score - 0.5) * 20  # -10~+10ì 
    score += int(vision_score_scaled)

    print(f"vision_score_scaled: {vision_score_scaled}")
    if vision_score_scaled > 3:
        feedback.append(f"âœ… ì „ë°˜ì ìœ¼ë¡œ ê¹”ë”í•œ ì´ë¯¸ì§€ì…ë‹ˆë‹¤ (ê°€ì  +{int(vision_score_scaled)}ì )")
    elif vision_score_scaled < -1:
        abs_score = int(-vision_score_scaled)
        if abs_score <= 3:
            msg = "ì¡°ê¸ˆ ì–´ìˆ˜ì„ í•©ë‹ˆë‹¤"
        elif abs_score <= 6:
            msg = "ê½¤ ì–´ìˆ˜ì„ í•©ë‹ˆë‹¤"
        elif abs_score <= 9:
            msg = "ë§¤ìš° ì–´ìˆ˜ì„ í•©ë‹ˆë‹¤"
        else:
            msg = "ì‹¬ê°í•˜ê²Œ ì–´ìˆ˜ì„ í•©ë‹ˆë‹¤"
        feedback.append(f"ğŸ“Œ ì´ë¯¸ì§€ ì „ë°˜ì´ {msg} (ê°ì  {abs_score}ì )")
        feedback.append("ğŸ’¡ ì •ë¦¬ íŒ: ë¬¼ê±´ë“¤ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•˜ê³ , ë¹„ìŠ·í•œ ë¬¼ê±´ë¼ë¦¬ ëª¨ì•„ë‘ì„¸ìš”. (ë°°ê²½ë„ í‰ê°€ì— í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì±…ìƒë§Œ ë³´ì´ê²Œ ì°ì–´ì£¼ì„¸ìš”.)")
    else:
        feedback.append("ğŸŸ° ì´ë¯¸ì§€ ë³µì¡ë„ëŠ” í‰ê·  ìˆ˜ì¤€ì…ë‹ˆë‹¤ (Â±0ì )")

    # ë¬¼ê±´ ì°¨ì§€ ë¹„ìœ¨
    if len(detections) > 0:
        img_h, img_w = image_gray.shape
        object_area = 0
        for d in detections:
            x1, y1, x2, y2 = map(int, d['box'])
            x1 = np.clip(x1, 0, img_w)
            x2 = np.clip(x2, 0, img_w)
            y1 = np.clip(y1, 0, img_h)
            y2 = np.clip(y2, 0, img_h)
            area = max(0, x2 - x1) * max(0, y2 - y1)
            object_area += area
        
        area_ratio = object_area / (img_h * img_w)
        if area_ratio > 0.40:
            score -= 10
            feedback.append(f"ğŸ“Œ ë¬¼ê±´ì´ ë„ˆë¬´ ë§ì€ ìë¦¬ë¥¼ ì°¨ì§€í•˜ê³  ìˆì–´ìš”! (ê°ì  -10, ë¹„ìœ¨: {area_ratio:.1%})")
            feedback.append("ğŸ’¡ ì •ë¦¬ íŒ: ì±…ìƒì˜ 40% ì´ìƒì„ ë¬¼ê±´ì´ ì°¨ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ë¬¼ê±´ì„ ì¹˜ìš°ê³  ì—¬ë°±ì„ í™•ë³´í•˜ì„¸ìš”.")
        elif area_ratio <= 0.3:
            score += 5
            feedback.append(f"âœ… ë¬¼ê±´ì´ ì ë‹¹í•œ ê³µê°„ë§Œ ì°¨ì§€í•˜ê³  ìˆì–´ìš”! (ê°€ì  +5, ë¹„ìœ¨: {area_ratio:.1%})")
        else:
            feedback.append(f"ğŸŸ° ë¬¼ê±´ ì°¨ì§€ ë¹„ìœ¨ì´ í‰ë²”í•©ë‹ˆë‹¤ (ë¹„ìœ¨: {area_ratio:.1%})")

    return max(0, min(100, score)), feedback

def make_feedback(score, num_objects):
    msg = ""
    if score >= 90:
        msg += "ğŸŸ¢ ì±…ìƒì´ ë§¤ìš° ê¹”ë”í•©ë‹ˆë‹¤.\n"
    elif score >= 70:
        msg += "ğŸŸ¡ ì •ëˆì€ ë˜ì—ˆìœ¼ë‚˜ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.\n"
    else:
        msg += "ğŸ”´ ì±…ìƒì´ ì–´ì§€ëŸ½ê³  ì •ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n"
    if num_objects < 5:
        msg += "âš ï¸ ê°ì²´ ìˆ˜ê°€ ì ì–´ ì‹ ë¢°ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
    return msg

model = YOLO("last.pt")

def analyze_desk(image):
    try:
        results = model(image, conf=0.25, iou=0.1)
        boxes = results[0].boxes.xyxy.cpu().numpy()
        classes = results[0].boxes.cls.cpu().numpy().astype(int)
        confs = results[0].boxes.conf.cpu().numpy()

        detections = []
        img = image.copy()
        for box, cls, conf in zip(boxes, classes, confs):
            label = NAMES[cls] if cls < len(NAMES) else str(cls)
            detections.append({'name': label, 'box': box, 'conf': conf})
            x1, y1, x2, y2 = map(int, box)
            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        score, feedback_list = score_desk(detections, gray, image)
        feedback_msg = make_feedback(score, len(detections))
        feedback_detail = "\n".join(feedback_list)
        final_feedback = f"ğŸ“Š ì ìˆ˜: {score}ì \n\n{feedback_msg}\nğŸ“‹ ìƒì„¸ í”¼ë“œë°±:\n{feedback_detail}"
        return img, final_feedback
    except Exception as e:
        error_msg = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        return image, error_msg

iface = gr.Interface(
    fn=analyze_desk,
    inputs=gr.Image(type="numpy", label="ì±…ìƒ ì‚¬ì§„ ì—…ë¡œë“œ"),
    outputs=[
        gr.Image(type="numpy", label="íƒì§€ ê²°ê³¼"),
        gr.Textbox(label="í”¼ë“œë°± ë° ì ìˆ˜", lines=10)
    ],
    title="ğŸ“ ì±…ìƒ ì •ëˆ ìƒíƒœ í‰ê°€",
    description="ì±…ìƒ ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ë©´ YOLO + ì‹œê°ì  ë³µì¡ë„ ê¸°ë°˜ìœ¼ë¡œ ì •ëˆ ì ìˆ˜ì™€ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.",
    examples=[],
    cache_examples=False
)

if __name__ == "__main__":
    iface.launch(share=True)
